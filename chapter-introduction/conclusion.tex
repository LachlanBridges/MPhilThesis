%!TEX root = ../thesis.tex




In this thesis, we have considered an existing one-dimensional foraging model and derived expressions for the efficiency of a Markov-modulated random walk strategy. We have shown how these can be discretised and solved numerically. Using these expressions, results for the optimal efficiency were found for a range of different strategies, and we have discussed how some existing strategies can be seen as a special case of our Markov-modulated random walk strategy. We have confirmed the conclusions of these, and in some cases been able to extend the results. We also constructed a simple two-dimensional model and found some basic results about the efficiency of destructive foraging. We now recap the main findings of our thesis, as well as describe some future work.

\paragraph{\Cref{sec:1Dmodel}: One-dimensional foraging model}
This chapter was perhaps the most important. We began with an existing one-dimensional foraging model and first considered a random walk search strategy, for which results already exist \cite{Bartumeus_2013}. We proved a number of these results, such as the expected total distance travelled to find a target. Although already known, we have placed these results on more rigorous grounding. For example, we have shown explicitly that the operator norm of the integral operator we use is less than one, and hence convergence of the Neumann series is guaranteed. Further, we also found a separate expression for the expected total travel length which does not require a known starting location.

We then considered a Markov-modulated random walk search strategy, and derived analogues to the existing expressions for the random walk search strategy, such as the expected total travel length and the expected number of steps to find a target. More than just switching step-length distributions, we also allow the Markov chain to change the forager's radius of vision. One limitation of our expressions is that we are required to assume that the transition matrix does not depend on the forager's current location. A transition matrix that was a function of the forager's current location would actually allow us to investigate location-aware foragers, such as those of \cite{Nolting_2013}. This would represent a forager that changes strategy depending on where it is relative to a food patch, for which there is empirical evidence of this occurring \cite{Patterson_2017}. Another limitation is that our derivation requires a step-length distribution that is symmetric about zero. If we could consider non-symmetric step-length distributions, we could investigate the effects of an orientational memory by making movements in one direction more likely than the other.

\paragraph{\Cref{sec:1d_discrete}: Discretisation of the one-dimensional search space}
In this chapter, we discretised the one-dimensional model from \Cref{sec:1Dmodel}. Following a similar methodology as earlier, we once again use an operator, which we showed could be represented by a matrix $A$. We find discretised analogues of all of the important results from \Cref{sec:1Dmodel}, and discussed how they could be solved numerically in Matlab. The work of this chapter also helps to demonstrate some of the difficulties that arose in \cref{sec:1Dmodel}, since we are able to look at the problem in terms of matrices instead of integrals. In particular, the issue with finding the cost incurred in a single state rather than all states is easier to understand from the perspective of a discrete search space.


\paragraph{\Cref{sec:1d_results}: Results for the one-dimensional model}
In \cref{sec:1d_results}, we implemented the discrete expressions for the expected total area that we derived in \cref{sec:1d_discrete}, and investigated the efficiency of a range of different strategies. 
In \cref{sec:1dMMRW_nonMM}, we considered the unmodulated random walk, which is a special case of the Markov-modulated random walk, when the Markov chain has only a single state. We plotted the efficiency for four different step-length distributions: unbounded power-law, bounded power-law, unbounded exponential, and bounded exponential. For destructive foraging, the optimal strategy was to perform ballistic motion. For non-destructive foraging, ballistic motion was best for the unbounded and bounded exponential distributions, but for the two power-law distributions, a L\'{e}vy walk with $\mu \approx 2$ provided a higher efficiency. These results are all consistent with previous results found throughout the literature. 

We also investigated the effect that the upper bound has on the search efficiency. For both the exponential and power-law distributions, the upper bound caused a lower efficiency, and the lower the upper bound was, the worse the efficiency became.

In \cref{sec:1dMMRW_GUT}, we showed how giving-up time strategies could be modelled using our Markov-modulated random walk strategy, by constructing the transition matrix to have an absorbing state. Thus, we reasoned that we could use our Markov-modulated random walk model to consider any discrete phase-type distribution for the giving-up time, and could in theory construct our transition matrix in such a way as to approximate any giving-up time distribution, since the phase-type distribution is dense in the field of all positive-valued distributions. However, for accurate approximations, we may require using Markov chains with a large number of states, which quickly becomes computationally infeasible.

We considered a Brownian motion giving-up into a ballistic motion, and found the giving-up time parameter which optimised the efficiency. Our results are relatively close to the existing results by Plank and James \cite{Plank_2008}, given the significantly different approaches taken between us and them. 

Next, we fixed the giving-up time and investigated the optimal choice of step-length distribution parameters. We found that a Brownian motion that gives up and performs a L\'{e}vy flight is the most efficient, matching the results of Reynolds \cite{Reynolds_2009_adaptive}. The optimal parameter of the L\'{e}vy flight depends on the giving-up time, and the faster a forager gives up the larger $\mu$ should be, with a maximum being $\mu \approx 2$, corresponding to when a forager instantly gives up which is essentially the same as using a single strategy.

Finally, for the giving-up time strategies, we optimised over both the giving-up time parameter and the step-length distribution parameters, for a two-state strategy. This combines the best of both of the two previously mentioned papers on giving-up time strategies. We found the optimal efficiency occurred when the forager began with a Brownian motion with $\mu$ as large as possible, and gave up after a geometrically distributed time ($p=0.0060$, mean of $166.6$) and performed a L\'{e}vy flight with $\mu_2 = 1.5868$. As far as we are aware, there is no existing literature that has optimised over both the step-length parameters and the giving-up time at the same time.

We also discussed how we could consider various vision-switching strategies, in \cref{sec:1dMMRW_VisionSwitching}. We outlined how models with hidden or hard-to-detect targets could be approximated using our model, although this is computationally infeasible for heavy-tailed distributions, making this not very useful. Simpler models in which the forager's radius of vision switches at various times, which perhaps represents switching between different weather conditions, can easily be modelled with our Markov-modulated random walk strategy. We plotted an example of this, showing how a switching vision skews the symmetrical plot of step-length parameters against searching efficiency for a forager with equal probabilities of switching between each state. Since a model like this is very animal-specific and dependent on the choice of parameters, we do not determine any results for the optimal vision-switching forager.

Finally, in \cref{sec:1dMMRW_all} we investigated the most general models thus far, the Markov-modulated random walk, with any parameters for the transition matrix, initial distribution and step-length parameters. We began with a one-state model, and explain how we can construct a $J$-state model, showing that this involves $J^2+J-1$ parameters to optimise over. We used Matlab's \emph{fmincon} function to determine the parameters that result in the optimal efficiency for the $1$-state, $2$-state, and $3$-state strategies. We also outlined a method for selecting a reasonable starting point for the optimisation, which uses the results of the previous solution.

The optimal solution for the $1$-state model simply reiterated the results that we found for the unmodulated random walk, which is $\mu = 1.8699$. For the $2$-state model, we found something more interesting. The optimal strategy was to begin with a Brownian motion before switching into a L\'{e}vy flight with $\mu=1.5868$, with the transition matrix having an absorbing state, and initial distribution $(1,0)$. This is exactly the optimal two-state giving-up time strategy found earlier. Thus, we are able to conclude that giving-up time strategies are optimal for $2$-state Markov-modulated random walks. For the $3$-state model, we once again find that a giving-up time strategy is optimal. The optimal transition matrix was found to be
\begin{equation*}
P = \begin{bmatrix}
0.9849  &  0 &   0.0151\\
0  &  1  &  0\\
0 &  0.0152   & 0.9848
\end{bmatrix},
\end{equation*}
with an initial probability vector $\vec{z_{0}} = (1,0,0)$, and distribution parameters $\vec{\mu} = (10, 1.3908,    2.4368)$. This strategy once again corresponds to a giving-up time strategy that begins following a Brownian motion, before giving-up into a L\'{e}vy flight, with parameter $\mu = 2.4368$. Then, after some further time, the forager gives up again, and switches to a L\'{e}vy flight with $\mu = 1.3908$, which will somewhat resemble a ballistic path, due to the small $\mu$ parameter. The mean of the both geometrically distributed giving-up times is approximately $66$ steps. 

We hypothesise that the optimal intermittent strategies will always be giving-up time strategies. Further, we suspect that a $J$-state model will involve $J-1$ times at which the forager gives up into a different state, with no switching back to previous states. The $\mu$ parameters of these states will decrease each time a forager gives up. If this hypothesis is correct, it may perhaps also point to a potential optimal strategy in which a forager begins following a Brownian motion and $\mu$ decays continuously over time.

\paragraph{\Cref{sec:2dmodel}: Two-dimensional model}
In \cref{sec:2dmodel} we constructed a simple two-dimensional foraging model with targets distributed with complete spatial randomness. In order to find some analytic results, we introduced a simplifying assumption about target regeneration in areas that have already been explored. When targets can regenerate in areas that were explored previously in the step immediately prior, we call it the zeroth-order model. Similarly, if targets can regenerate after a delay of one step, we call this the first-order model. The case of no regeneration is called the infinite-order model, which involves no approximation. 

We found some analytic results for the efficiency of a random walk search strategy in the zeroth-order, which happens to not depend on the choice of step-length distribution at all. This result also corresponds to an upper bound for the efficiency of a strategy. For the first-order model, there is overlap between consecutive steps so we discussed the area of overlap as a function of the turning-angle and the step-lengths. We found approximate expressions for the area of a step, and use Monte Carlo simulations to ensure these are accurate. Ultimately, the expressions for the area are still too complicated to arrive at analytic expressions for the efficiency under the first-order model, though we can conclude that distributions with larger step-lengths will offer the highest efficiency, matching existing results. 

Finally, we simulated the zeroth-order, first-order, and infinite-order model numerically, and show that the first-order model does not actually offer a very good approximation of the infinite-order model anyway. We also argued that for analytic results to be found, much simpler two-dimensional models should be considered, such as lattice models.

\section{Future Work}
As discussed above, our work in \Cref{sec:1Dmodel} required an assumption that the transition matrix did not depend on the forager's current location, preventing us from considering location-aware foragers. However, our model may perhaps still be able to be used in investigating location-aware foragers, by making some extensions. In the location-aware forager of Nolting \cite{Nolting_2013}, the search area is divided into three sections, and the forager would use a different strategy in the left and right sections than the one it used in the middle section. One possible way we could consider this is by actually considering this as multiple separate searches, with the boundaries of the middle section representing a change of strategy rather than a food target. However, this would also require changes to the way our truncation is performed, as well as knowing more about whether it is the left or the right boundary that is found throughout a search.

In terms of considering non-symmetric step-length distributions in order to introduce an orientational memory, there are some possible avenues to explore. There are some key moments throughout \Cref{sec:1Dmodel} at which the symmetry is needed, in particular, after the introduction of the Dirac delta function. However, recall that we also found expressions that did not require a known starting location, so perhaps the symmetry assumption may be relaxed. This will require more investigation, and  will also need to consider how this will affect the discretisation of the step-length distributions, as well as any restrictions introduced to ensure that the operator norm is less than unity.

Recall from \cref{sec:1d_results} that we outlined a way to approximate the hidden targets model, although it was not adequate. This is because even when our forager has a vision radius of $0$, it may still reach the food by running into the target. To alleviate this problem, we can introduce periodic boundary conditions on the interval $[0,\lambda]$. Then, a food target would only be found if the forager landed with the food within its radius of vision, meaning it could skip over targets. To do this, we would have to adjust our expression for $\rho_n(x_n)$, and we now have to consider the previous location $\rho_{n-1}(x_{n-1})$ coming from any other interval, not just the current interval. This will introduce an infinite summation into the $\rho_n(x)$ recursive relation, which will not necessarily converge quickly when we consider heavy-tailed step-length distributions. 

In \cref{sec:1d_results}, apart from the unmodulated strategies, we only considered unbounded power-law distributions. This is because the other step-length distributions we considered offered worse performance in the unmodulated case. However, they may still offer better efficiency when considering switching strategies. No further extensions are required to test these out, though there are many possible combinations to consider, which can take a fairly long time to run. Similarly, we can consider $4$-state and higher Markov-modulated random walk models, which are also possible using our existing work, though slow computationally.

Further, we could also consider other distributions than the four we have investigated throughout this thesis. Our reasoning in choosing these four was that they are the only four that have been commonly used throughout the literature. However, more recent papers (e.g \cite{Bertrand_2015}) have considered a generalized Pareto distribution, of which Normal, exponential, and power-law distributions are special cases.

In terms of two-dimensional models, further work should first focus on simulations. Further investigation should be made into which one-dimensional results hold in higher dimensions. The model we outline in \cref{sec:2dmodel} could also be adjusted to account for non-destructive foraging in a few different ways. For example, rather than have a fully homogeneous spatial Poisson process, there could be different regions, representing areas that are either close or far from a previously visited target. Alternatively, the density of targets may be set high initially and made to decay over time, to represent the chance of reaching the previously visited target diminishing.
